{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델매개변수 최적화 하기\n",
    "- 매개변수의 오류에 대한 그래디언트를 계산하고 경사하강법을통해 매개변수를 최적화한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "#데이터와 레이블 확인\n",
    "training_data.train_data.shape  #torch.Size([60000, 28, 28])\n",
    "training_data.train_labels.shape  #torch.Size([60000])\n",
    "\n",
    "# n = 1\n",
    "# plt.imshow(training_data.train_data[n], cmap='gray')\n",
    "# print(training_data.train_labels[n]) # 0번 레이블이다 = 상의\n",
    "\n",
    "# 데이터 로더 만들기\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# 형태확인 64개의 데이터, 1채널=흑백 이미지는 28*28 크기\n",
    "next(iter(train_dataloader))[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset.train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_leakyrelu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 신경망 정의 \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_leakyrelu_stack = nn.Sequential(\n",
    "            # 입력 784채널 출력 512채널\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512,10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_leakyrelu_stack(x)\n",
    "        return logits\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model =  NeuralNetwork()\n",
    "model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noptimizer.zero_grad()를 호출하여 모델 매개변수의 변화도를 재설정\\nloss.backwards()를 호출하여 예측 손실(prediction loss)을 역전파\\n변화도 계산후 optimizer.step() 호출하여 역전파 단계에서 수집된 변화도로 매개변수를 조정\\n'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''hyper params\n",
    "epoch: 데이터셋 전체를 학습에 반복시키는 횟수\n",
    "batch_size: 매개변수를 업데이트 하기전 신경망에 전파할 데이터수 \n",
    "learning_rate: 각 배치/에폭에서 모델의 매개변수를 조절하는 비율이다\n",
    "    값이작으면 학습속도가 느려지고,크면 예측 할수없는 동작이 발생(우물점프?) \n",
    "'''\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "# 6만개를 64개씩 나누어서 학습을 시키고 \n",
    "# 6만개를 총 5번 반복해서 학습한다는 뜻이다\n",
    "\n",
    "# 손실함수 초기화\n",
    "# 크로스엔트로피: 다중분류일때 사용, 소프트맥스와 nll을 합친것이다\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저는 매개변수를 최적화하는 알고리즘을 설정하는것이다\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "optimizer.zero_grad()를 호출하여 모델 매개변수의 변화도를 재설정\n",
    "loss.backwards()를 호출하여 예측 손실(prediction loss)을 역전파\n",
    "변화도 계산후 optimizer.step() 호출하여 역전파 단계에서 수집된 변화도로 매개변수를 조정\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위를 train_loop와 test_loop로 정리하면\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    # dataloader = train_dataloader\n",
    "    size = len(dataloader.dataset)\n",
    "    # 배치가 몇번째인지, 한배치의 갯수에 맞게 X,y 각각 여러개\n",
    "    for batch,(X,y) in enumerate(dataloader):\n",
    "        # 여기서 예측과 손실을 계산한다\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # 역전파 실행\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  # 그래디언트 계산\n",
    "        optimizer.step()  # 계산된 그래디언트로 매개변수 조정\n",
    "        \n",
    "        if batch%100 ==0:\n",
    "            loss, current =loss.item(), (batch+1) *len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.304832 [   64/60000]\n",
      "loss: 2.278584 [ 6464/60000]\n",
      "loss: 2.273233 [12864/60000]\n",
      "loss: 2.273821 [19264/60000]\n",
      "loss: 2.274278 [25664/60000]\n",
      "loss: 2.231444 [32064/60000]\n",
      "loss: 2.230968 [38464/60000]\n",
      "loss: 2.173136 [44864/60000]\n",
      "loss: 2.195565 [51264/60000]\n",
      "loss: 2.146554 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.0%, Avg loss: 2.155211 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.168530 [   64/60000]\n",
      "loss: 2.131942 [ 6464/60000]\n",
      "loss: 2.097974 [12864/60000]\n",
      "loss: 2.118147 [19264/60000]\n",
      "loss: 2.056571 [25664/60000]\n",
      "loss: 2.023834 [32064/60000]\n",
      "loss: 2.005002 [38464/60000]\n",
      "loss: 1.969051 [44864/60000]\n",
      "loss: 1.923591 [51264/60000]\n",
      "loss: 1.921978 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 1.879166 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.915569 [   64/60000]\n",
      "loss: 1.891047 [ 6464/60000]\n",
      "loss: 1.755988 [12864/60000]\n",
      "loss: 1.775445 [19264/60000]\n",
      "loss: 1.781431 [25664/60000]\n",
      "loss: 1.682112 [32064/60000]\n",
      "loss: 1.656848 [38464/60000]\n",
      "loss: 1.544843 [44864/60000]\n",
      "loss: 1.518573 [51264/60000]\n",
      "loss: 1.506685 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.507125 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.469641 [   64/60000]\n",
      "loss: 1.368676 [ 6464/60000]\n",
      "loss: 1.448222 [12864/60000]\n",
      "loss: 1.364289 [19264/60000]\n",
      "loss: 1.296886 [25664/60000]\n",
      "loss: 1.282591 [32064/60000]\n",
      "loss: 1.287576 [38464/60000]\n",
      "loss: 1.289419 [44864/60000]\n",
      "loss: 1.331326 [51264/60000]\n",
      "loss: 1.210381 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.242920 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.263229 [   64/60000]\n",
      "loss: 1.242933 [ 6464/60000]\n",
      "loss: 1.179399 [12864/60000]\n",
      "loss: 1.256194 [19264/60000]\n",
      "loss: 1.376378 [25664/60000]\n",
      "loss: 1.098577 [32064/60000]\n",
      "loss: 1.167649 [38464/60000]\n",
      "loss: 1.130149 [44864/60000]\n",
      "loss: 1.147792 [51264/60000]\n",
      "loss: 1.059331 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 1.081195 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.056489 [   64/60000]\n",
      "loss: 0.980134 [ 6464/60000]\n",
      "loss: 1.118245 [12864/60000]\n",
      "loss: 0.954800 [19264/60000]\n",
      "loss: 1.125352 [25664/60000]\n",
      "loss: 0.930738 [32064/60000]\n",
      "loss: 1.023855 [38464/60000]\n",
      "loss: 0.968937 [44864/60000]\n",
      "loss: 1.015534 [51264/60000]\n",
      "loss: 0.928496 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.977250 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.938766 [   64/60000]\n",
      "loss: 1.014518 [ 6464/60000]\n",
      "loss: 0.881804 [12864/60000]\n",
      "loss: 0.963133 [19264/60000]\n",
      "loss: 0.817552 [25664/60000]\n",
      "loss: 1.029036 [32064/60000]\n",
      "loss: 0.992132 [38464/60000]\n",
      "loss: 0.794164 [44864/60000]\n",
      "loss: 0.930363 [51264/60000]\n",
      "loss: 1.018165 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.906771 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.067224 [   64/60000]\n",
      "loss: 0.884202 [ 6464/60000]\n",
      "loss: 0.854152 [12864/60000]\n",
      "loss: 0.786014 [19264/60000]\n",
      "loss: 0.918548 [25664/60000]\n",
      "loss: 0.921769 [32064/60000]\n",
      "loss: 0.754954 [38464/60000]\n",
      "loss: 0.990913 [44864/60000]\n",
      "loss: 0.889563 [51264/60000]\n",
      "loss: 0.843407 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.856591 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.858504 [   64/60000]\n",
      "loss: 0.962112 [ 6464/60000]\n",
      "loss: 0.861887 [12864/60000]\n",
      "loss: 0.928459 [19264/60000]\n",
      "loss: 0.890667 [25664/60000]\n",
      "loss: 0.933936 [32064/60000]\n",
      "loss: 0.883999 [38464/60000]\n",
      "loss: 0.894939 [44864/60000]\n",
      "loss: 0.817154 [51264/60000]\n",
      "loss: 0.790503 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.817446 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.712574 [   64/60000]\n",
      "loss: 0.805324 [ 6464/60000]\n",
      "loss: 0.762836 [12864/60000]\n",
      "loss: 0.876188 [19264/60000]\n",
      "loss: 0.632151 [25664/60000]\n",
      "loss: 0.813150 [32064/60000]\n",
      "loss: 0.843099 [38464/60000]\n",
      "loss: 0.706557 [44864/60000]\n",
      "loss: 0.727396 [51264/60000]\n",
      "loss: 0.724156 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.787878 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 하기\n",
    "torch.save(model.state_dict(), 'model_07.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
